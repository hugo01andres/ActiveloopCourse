{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model that can answer many questions at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# load the model\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "# user question\n",
    "question = \"What is the capital city of France?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris\n",
      "Tokens Used: 20\n",
      "\tPrompt Tokens: 19\n",
      "\tCompletion Tokens: 1\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0006299999999999999\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# Create a LLMChain in easy words give the model a prompt and it will generate the rest\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    ")\n",
    "# ask the user question about the capital of France\n",
    "with get_openai_callback() as cb:\n",
    "    print(llm_chain.run(question))\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[ChatGeneration(text='Paris', generation_info=None, message=AIMessage(content='Paris', additional_kwargs={}, example=False))], [ChatGeneration(text='The Blue Whale', generation_info=None, message=AIMessage(content='The Blue Whale', additional_kwargs={}, example=False))], [ChatGeneration(text='Nitrogen', generation_info=None, message=AIMessage(content='Nitrogen', additional_kwargs={}, example=False))], [ChatGeneration(text='Yellow', generation_info=None, message=AIMessage(content='Yellow', additional_kwargs={}, example=False))]] llm_output={'token_usage': {'prompt_tokens': 78, 'completion_tokens': 7, 'total_tokens': 85}, 'model_name': 'gpt-4'} run=RunInfo(run_id=UUID('605dcaae-35dc-47ee-9500-ed223c430cfd'))\n",
      "Tokens Used: 85\n",
      "\tPrompt Tokens: 78\n",
      "\tCompletion Tokens: 7\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00276\n"
     ]
    }
   ],
   "source": [
    "qa = [\n",
    "    {'question': \"What is the capital city of France?\"},\n",
    "    {'question': \"What is the largest mammal on Earth?\"},\n",
    "    {'question': \"Which gas is most abundant in Earth's atmosphere?\"},\n",
    "    {'question': \"What color is a ripe banana?\"}\n",
    "]\n",
    "with get_openai_callback() as cb:\n",
    "    res = llm_chain.generate(qa)\n",
    "    print( res )\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital city of France is Paris.\n",
      "The largest mammal on Earth is the Blue Whale.\n",
      "The gas most abundant in Earth's atmosphere is Nitrogen.\n",
      "A ripe banana is yellow.\n",
      "Tokens Used: 91\n",
      "\tPrompt Tokens: 54\n",
      "\tCompletion Tokens: 37\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0038399999999999997\n"
     ]
    }
   ],
   "source": [
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "long_prompt = PromptTemplate(template=multi_template, input_variables=[\"questions\"])\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "qs_str = (\n",
    "    \"What is the capital city of France?\\n\" +\n",
    "    \"What is the largest mammal on Earth?\\n\" +\n",
    "    \"Which gas is most abundant in Earth's atmosphere?\\n\" +\n",
    "\t\t\"What color is a ripe banana?\\n\"\n",
    ")\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    print(llm_chain.run(qs_str))\n",
    "    print(cb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
